{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Datatransformation"]},{"cell_type":"markdown","metadata":{},"source":["* **Libraries required**"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sklearnNote: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["  error: subprocess-exited-with-error\n","  \n","  × Getting requirements to build wheel did not run successfully.\n","  │ exit code: 1\n","  ╰─> [18 lines of output]\n","      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n","      rather than 'sklearn' for pip commands.\n","      \n","      Here is how to fix this error in the main use cases:\n","      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n","      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n","        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n","      - if the 'sklearn' package is used by one of your dependencies,\n","        it would be great if you take some time to track which package uses\n","        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n","      - as a last resort, set the environment variable\n","        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n","      \n","      More information is available at\n","      https://github.com/scikit-learn/sklearn-pypi-package\n","      \n","      If the previous advice does not cover your use case, feel free to report it at\n","      https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n","      [end of output]\n","  \n","  note: This error originates from a subprocess, and is likely not a problem with pip.\n","error: subprocess-exited-with-error\n","\n","× Getting requirements to build wheel did not run successfully.\n","│ exit code: 1\n","╰─> See above for output.\n","\n","note: This error originates from a subprocess, and is likely not a problem with pip.\n","\n","[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["\n","  Using cached sklearn-0.0.post10.tar.gz (3.6 kB)\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'error'\n"]}],"source":["pip install sklearn\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:48:58.737139Z","iopub.status.busy":"2022-09-30T03:48:58.736756Z","iopub.status.idle":"2022-09-30T03:48:59.953319Z","shell.execute_reply":"2022-09-30T03:48:59.951742Z","shell.execute_reply.started":"2022-09-30T03:48:58.737109Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'sklearn'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32md:\\Projects\\ytToolkit\\Sentiment_analysis\\youtube-comments-sentiment-analyser.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/ytToolkit/Sentiment_analysis/youtube-comments-sentiment-analyser.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/ytToolkit/Sentiment_analysis/youtube-comments-sentiment-analyser.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# import matplotlib.pyplot as plt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/ytToolkit/Sentiment_analysis/youtube-comments-sentiment-analyser.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# %matplotlib inline\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/ytToolkit/Sentiment_analysis/youtube-comments-sentiment-analyser.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# import os\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/ytToolkit/Sentiment_analysis/youtube-comments-sentiment-analyser.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/ytToolkit/Sentiment_analysis/youtube-comments-sentiment-analyser.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Import functions for data preprocessing & data preparation\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/ytToolkit/Sentiment_analysis/youtube-comments-sentiment-analyser.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/ytToolkit/Sentiment_analysis/youtube-comments-sentiment-analyser.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m resample\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/ytToolkit/Sentiment_analysis/youtube-comments-sentiment-analyser.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m CountVectorizer\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"]}],"source":["#Libraries \n","import numpy as np\n","import pandas as pd \n","# import matplotlib.pyplot as plt\n","# %matplotlib inline\n","# import os\n","\n","# Import functions for data preprocessing & data preparation\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils import resample\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer, LancasterStemmer\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","import string\n","from string import punctuation\n","import nltk\n","import re"]},{"cell_type":"markdown","metadata":{},"source":["* **Read data**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:50:08.217064Z","iopub.status.busy":"2022-09-30T03:50:08.216624Z","iopub.status.idle":"2022-09-30T03:50:08.259185Z","shell.execute_reply":"2022-09-30T03:50:08.258416Z","shell.execute_reply.started":"2022-09-30T03:50:08.217018Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv('../input/blackadam-trailer-comments/comments.csv')\n","data.columns\n","data1=data.drop(['Unnamed: 0','Likes','Time','user','UserLink'],axis=1)\n","data1"]},{"cell_type":"markdown","metadata":{},"source":["* **Data labelling**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:51:34.155339Z","iopub.status.busy":"2022-09-30T03:51:34.154902Z","iopub.status.idle":"2022-09-30T03:51:34.708323Z","shell.execute_reply":"2022-09-30T03:51:34.70722Z","shell.execute_reply.started":"2022-09-30T03:51:34.155295Z"},"trusted":true},"outputs":[],"source":["nltk.download('vader_lexicon')\n","sentiments = SentimentIntensityAnalyzer()\n","data1[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data1[\"Comment\"]]\n","data1[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data1[\"Comment\"]]\n","data1[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data1[\"Comment\"]]\n","data1['Compound'] = [sentiments.polarity_scores(i)[\"compound\"] for i in data1[\"Comment\"]]\n","score = data1[\"Compound\"].values\n","sentiment = []\n","for i in score:\n","    if i >= 0.05 :\n","        sentiment.append('Positive')\n","    elif i <= -0.05 :\n","        sentiment.append('Negative')\n","    else:\n","        sentiment.append('Neutral')\n","data1[\"Sentiment\"] = sentiment\n","data1.head()"]},{"cell_type":"markdown","metadata":{},"source":["* **Final data**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:51:55.202179Z","iopub.status.busy":"2022-09-30T03:51:55.201795Z","iopub.status.idle":"2022-09-30T03:51:55.214681Z","shell.execute_reply":"2022-09-30T03:51:55.213622Z","shell.execute_reply.started":"2022-09-30T03:51:55.202148Z"},"trusted":true},"outputs":[],"source":["data2=data1.drop(['Positive','Negative','Neutral','Compound'],axis=1)\n","data2.head()"]},{"cell_type":"markdown","metadata":{},"source":["* **Data transformation**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:52:28.600623Z","iopub.status.busy":"2022-09-30T03:52:28.600169Z","iopub.status.idle":"2022-09-30T03:52:28.612625Z","shell.execute_reply":"2022-09-30T03:52:28.611106Z","shell.execute_reply.started":"2022-09-30T03:52:28.600591Z"},"trusted":true},"outputs":[],"source":["stop_words = stopwords.words('english')\n","porter_stemmer = PorterStemmer()\n","lancaster_stemmer = LancasterStemmer() \n","snowball_stemer = SnowballStemmer(language=\"english\")\n","lzr = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:52:40.939078Z","iopub.status.busy":"2022-09-30T03:52:40.938632Z","iopub.status.idle":"2022-09-30T03:52:40.948809Z","shell.execute_reply":"2022-09-30T03:52:40.947431Z","shell.execute_reply.started":"2022-09-30T03:52:40.939016Z"},"trusted":true},"outputs":[],"source":["def text_processing(text):   \n","    # convert text into lowercase\n","    text = text.lower()\n","\n","    # remove new line characters in text\n","    text = re.sub(r'\\n',' ', text)\n","    \n","    # remove punctuations from text\n","    text = re.sub('[%s]' % re.escape(punctuation), \"\", text)\n","    \n","    # remove references and hashtags from text\n","    text = re.sub(\"^a-zA-Z0-9$,.\", \"\", text)\n","    \n","    # remove multiple spaces from text\n","    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n","    \n","    # remove special characters from text\n","    text = re.sub(r'\\W', ' ', text)\n","\n","    text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])\n","    \n","    # stemming using porter stemmer from nltk package - msh a7sn 7aga - momken: lancaster, snowball\n","    # text=' '.join([porter_stemmer.stem(word) for word in word_tokenize(text)])\n","    # text=' '.join([lancaster_stemmer.stem(word) for word in word_tokenize(text)])\n","    # text=' '.join([snowball_stemer.stem(word) for word in word_tokenize(text)])\n","    \n","    # lemmatizer using WordNetLemmatizer from nltk package\n","    text=' '.join([lzr.lemmatize(word) for word in word_tokenize(text)])\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:53:20.284745Z","iopub.status.busy":"2022-09-30T03:53:20.284248Z","iopub.status.idle":"2022-09-30T03:53:22.666461Z","shell.execute_reply":"2022-09-30T03:53:22.66534Z","shell.execute_reply.started":"2022-09-30T03:53:20.284699Z"},"trusted":true},"outputs":[],"source":["nltk.download('omw-1.4')\n","data_copy = data2.copy()\n","data_copy.Comment = data_copy.Comment.apply(lambda text: text_processing(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:53:34.356063Z","iopub.status.busy":"2022-09-30T03:53:34.355651Z","iopub.status.idle":"2022-09-30T03:53:34.363193Z","shell.execute_reply":"2022-09-30T03:53:34.361939Z","shell.execute_reply.started":"2022-09-30T03:53:34.356031Z"},"trusted":true},"outputs":[],"source":["le = LabelEncoder()\n","data_copy['Sentiment'] = le.fit_transform(data_copy['Sentiment'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:53:44.962046Z","iopub.status.busy":"2022-09-30T03:53:44.96161Z","iopub.status.idle":"2022-09-30T03:53:44.974282Z","shell.execute_reply":"2022-09-30T03:53:44.973335Z","shell.execute_reply.started":"2022-09-30T03:53:44.96201Z"},"trusted":true},"outputs":[],"source":["processed_data = {\n","    'Sentence':data_copy.Comment,\n","    'Sentiment':data_copy['Sentiment']\n","}\n","\n","processed_data = pd.DataFrame(processed_data)\n","processed_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:53:56.635486Z","iopub.status.busy":"2022-09-30T03:53:56.635065Z","iopub.status.idle":"2022-09-30T03:53:56.647536Z","shell.execute_reply":"2022-09-30T03:53:56.646225Z","shell.execute_reply.started":"2022-09-30T03:53:56.635454Z"},"trusted":true},"outputs":[],"source":["processed_data['Sentiment'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["* **Balancing data**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:54:17.338941Z","iopub.status.busy":"2022-09-30T03:54:17.338474Z","iopub.status.idle":"2022-09-30T03:54:17.352555Z","shell.execute_reply":"2022-09-30T03:54:17.351645Z","shell.execute_reply.started":"2022-09-30T03:54:17.338906Z"},"trusted":true},"outputs":[],"source":["df_neutral = processed_data[(processed_data['Sentiment']==1)] \n","df_negative = processed_data[(processed_data['Sentiment']==0)]\n","df_positive = processed_data[(processed_data['Sentiment']==2)]\n","\n","# upsample minority classes\n","df_negative_upsampled = resample(df_negative, \n","                                 replace=True,    \n","                                 n_samples= 205, \n","                                 random_state=42)  \n","\n","df_neutral_upsampled = resample(df_neutral, \n","                                 replace=True,    \n","                                 n_samples= 205, \n","                                 random_state=42)  \n","\n","\n","# Concatenate the upsampled dataframes with the neutral dataframe\n","final_data = pd.concat([df_negative_upsampled,df_neutral_upsampled,df_positive])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:54:25.028719Z","iopub.status.busy":"2022-09-30T03:54:25.028149Z","iopub.status.idle":"2022-09-30T03:54:25.040231Z","shell.execute_reply":"2022-09-30T03:54:25.039104Z","shell.execute_reply.started":"2022-09-30T03:54:25.028658Z"},"trusted":true},"outputs":[],"source":["final_data['Sentiment'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:54:31.528819Z","iopub.status.busy":"2022-09-30T03:54:31.528441Z","iopub.status.idle":"2022-09-30T03:54:31.536516Z","shell.execute_reply":"2022-09-30T03:54:31.535377Z","shell.execute_reply.started":"2022-09-30T03:54:31.528789Z"},"trusted":true},"outputs":[],"source":["corpus = []\n","for sentence in final_data['Sentence']:\n","    corpus.append(sentence)\n","corpus[0:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:54:40.960425Z","iopub.status.busy":"2022-09-30T03:54:40.959872Z","iopub.status.idle":"2022-09-30T03:54:40.999093Z","shell.execute_reply":"2022-09-30T03:54:40.997735Z","shell.execute_reply.started":"2022-09-30T03:54:40.960344Z"},"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer(max_features=1500)\n","X = cv.fit_transform(corpus).toarray()\n","y = final_data.iloc[:, -1].values"]},{"cell_type":"markdown","metadata":{},"source":["* **Machine learning model**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:55:26.200338Z","iopub.status.busy":"2022-09-30T03:55:26.199895Z","iopub.status.idle":"2022-09-30T03:55:26.22859Z","shell.execute_reply":"2022-09-30T03:55:26.227715Z","shell.execute_reply.started":"2022-09-30T03:55:26.200296Z"},"trusted":true},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n","classifier = GaussianNB()\n","classifier.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["* **Evaluation**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:56:02.928287Z","iopub.status.busy":"2022-09-30T03:56:02.927763Z","iopub.status.idle":"2022-09-30T03:56:02.945251Z","shell.execute_reply":"2022-09-30T03:56:02.944025Z","shell.execute_reply.started":"2022-09-30T03:56:02.928241Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, accuracy_score\n","y_pred = classifier.predict(X_test)\n","cm = confusion_matrix(y_test, y_pred)\n","cm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-30T03:57:53.295855Z","iopub.status.busy":"2022-09-30T03:57:53.295473Z","iopub.status.idle":"2022-09-30T03:57:53.302485Z","shell.execute_reply":"2022-09-30T03:57:53.301343Z","shell.execute_reply.started":"2022-09-30T03:57:53.295823Z"},"trusted":true},"outputs":[],"source":["nb_score = accuracy_score(y_test, y_pred)\n","print('accuracy',nb_score)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["**Hope you like my notebooks,Your comments will add value to the notebook,Please comment down the improvements!! and upvote if you liked it.**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
